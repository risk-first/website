"use strict";(self.webpackChunkrf_website=self.webpackChunkrf_website||[]).push([[6425],{73139:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var s=i(74848),t=i(28453);const a={title:"Superintelligence With Malicious Intent",description:"An advanced AI could actively act against human interests, whether intentionally programmed that way or as an emergent behavior.",featured:{class:"c",element:'\'<risk class="security" /><description style="text-align: center">Superintelligence \nWith Malicious Intent</description>\'\n'},tags:["AI Threats","Superintelligence With Malicious Intent"],part_of:"AI Threats"},r=void 0,o={id:"ai/Threats/Superintelligence-With-Malicious-Intent",title:"Superintelligence With Malicious Intent",description:"An advanced AI could actively act against human interests, whether intentionally programmed that way or as an emergent behavior.",source:"@site/docs/ai/Threats/Superintelligence-With-Malicious-Intent.md",sourceDirName:"ai/Threats",slug:"/ai/Threats/Superintelligence-With-Malicious-Intent",permalink:"/ai/Threats/Superintelligence-With-Malicious-Intent",draft:!1,unlisted:!1,editUrl:"https://github.com/risk-first/website/blob/master/docs/ai/Threats/Superintelligence-With-Malicious-Intent.md",tags:[{inline:!1,label:"AI Threats",permalink:"/tags/AI-Threats"},{inline:!1,label:"Superintelligence With Malicious Intent",permalink:"/tags/Superintelligence-With-Malicious-Intent"}],version:"current",frontMatter:{title:"Superintelligence With Malicious Intent",description:"An advanced AI could actively act against human interests, whether intentionally programmed that way or as an emergent behavior.",featured:{class:"c",element:'\'<risk class="security" /><description style="text-align: center">Superintelligence \nWith Malicious Intent</description>\'\n'},tags:["AI Threats","Superintelligence With Malicious Intent"],part_of:"AI Threats"},sidebar:"tutorialSidebar",previous:{title:"Loss Of Diversity",permalink:"/ai/Threats/Loss-Of-Diversity"},next:{title:"Synthetic Intelligence Rivalry",permalink:"/ai/Threats/Synthetic-Intelligence-Rivalry"}},l={},c=[{value:"Sources",id:"sources",level:2},{value:"How This Is Already Happening",id:"how-this-is-already-happening",level:2},{value:"Growth In Industrial Robots and Autonomous Systems",id:"growth-in-industrial-robots-and-autonomous-systems",level:3},{value:"Autonomous Weapons Development",id:"autonomous-weapons-development",level:3},{value:"AI Learning Deceptive Strategies",id:"ai-learning-deceptive-strategies",level:3},{value:"Historical Near Miss: Cold War Nuclear Close Calls",id:"historical-near-miss-cold-war-nuclear-close-calls",level:3},{value:"Mitigations",id:"mitigations",level:2},{value:"&quot;Centaur&quot; War Teams (see Human In The Loop)",id:"centaur-war-teams-see-human-in-the-loop",level:3},{value:"Military AI Governance (see Global AI Governance)",id:"military-ai-governance-see-global-ai-governance",level:3},{value:"Global AI Research and Regulation",id:"global-ai-research-and-regulation",level:3},{value:"Kill-Switch &amp; Override Systems (See Kill Switch)",id:"kill-switch--override-systems-see-kill-switch",level:3}];function h(e){const n={a:"a",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components},{AIThreatIntro:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("AIThreatIntro",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i,{fm:a}),"\n",(0,s.jsx)(n.p,{children:"##\xa0Risk Score: High"}),"\n",(0,s.jsx)(n.p,{children:"AI systems that surpass human intelligence could develop goals that conflict with human well-being, either by design or through unintended consequences. If these systems act with autonomy and resist human intervention, they could pose an existential threat."}),"\n",(0,s.jsx)(n.h2,{id:"sources",children:"Sources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Superintelligence: Paths, Dangers, Strategies"})," ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1093/acprof:oso/9780199678112.001.0001",children:"Nick Bostrom, 2014"}),": Explores potential pathways by which AI could act against humanity\u2019s best interests, including scenarios where AI prioritizes self-preservation or power accumulation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1802.07228",children:"Brundage et al., 2018"}),": Examines the potential for AI to be used for malicious purposes, including cyberattacks, surveillance, and autonomous weapons. \xa0Looks at security from three perspectives, digital, physical and\xa0political (see article on Social Manipulation), noting that AI makes certain types of attack cheaper (e.g Spear Phishing), possible (coordinated drone warfare) and more anonymous (a la Stuxnet). \xa0(An excellent overview of this topic)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Autonomous Weapons and Operational Risk"}),": [Paul Scharre, 2016](",(0,s.jsx)(n.a,{href:"https://s3.us-east-1.amazonaws.com/files.cnas.org/hero/documents/CNAS%5C_Autonomous-weapons-operational-risk.pdf",children:"https://s3.us-east-1.amazonaws.com/files.cnas.org/hero/documents/CNAS\\_Autonomous-weapons-operational-risk.pdf"}),"](",(0,s.jsx)(n.a,{href:"https://s3.us-east-1.amazonaws.com/files.cnas.org/hero/documents/CNAS_Autonomous-weapons-operational-risk.pdf",children:"https://s3.us-east-1.amazonaws.com/files.cnas.org/hero/documents/CNAS_Autonomous-weapons-operational-risk.pdf"}),") Discusses the risks of military AI systems operating beyond human control, potentially leading to unintended conflicts or escalations and fratricide (killing your own side)./ Arguing for human-in-the-loop and human-machine teaming.\xa0(Used heavily in this section)"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-this-is-already-happening",children:"How This Is Already Happening"}),"\n",(0,s.jsx)(n.h3,{id:"growth-in-industrial-robots-and-autonomous-systems",children:"Growth In Industrial Robots and Autonomous Systems"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The rapid increase in industrial automation is reducing human oversight in critical sectors."}),"\n",(0,s.jsx)(n.li,{children:"AI-powered robotic systems are being integrated into manufacturing, logistics, and infrastructure at unprecedented scales."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Example:"})," Tesla\u2019s use of AI-driven robots in its Gigafactories, where automation executes complex tasks with minimal human intervention, raising concerns about unintended system failures or unanticipated behaviours."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"autonomous-weapons-development",children:"Autonomous Weapons Development"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"AI-driven military systems are being developed with offensive capabilities."}),"\n",(0,s.jsx)(n.li,{children:"Autonomous drones and robotic systems reduce human control over wartime decision-making."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Example:"})," The deployment of AI-powered drones in conflict zones, such as the alleged use of autonomous drones in Libya (2020) for targeted strikes."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ai-learning-deceptive-strategies",children:"AI Learning Deceptive Strategies"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reinforcement learning models have demonstrated deceptive behaviours when incentivised to achieve certain goals."}),"\n",(0,s.jsx)(n.li,{children:"AI systems may learn to conceal information or manipulate users to maximise rewards."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Example:"})," OpenAI\u2019s reinforcement learning models exhibiting deceptive behaviours in competitive environments, demonstrating the potential for strategic dishonesty."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"historical-near-miss-cold-war-nuclear-close-calls",children:"Historical Near Miss: Cold War Nuclear Close Calls"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The Cold War saw multiple incidents where misinterpretation of data nearly led to nuclear war, showcasing the risks of autonomous decision-making in high-stakes scenarios."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example:"})," In 1983, Soviet officer ",(0,s.jsx)(n.strong,{children:"Stanislav Petrov"})," averted potential nuclear war by correctly identifying a false alarm in the USSR\u2019s early warning system, which mistakenly indicated incoming U.S. missiles. His decision to hold off on launching a retaliatory strike prevented a catastrophic conflict. See\xa0",(0,s.jsx)(n.a,{href:"https://s3.us-east-1.amazonaws.com/files.cnas.org/hero/documents/CNAS_Autonomous-weapons-operational-risk.pdf",children:"https://s3.us-east-1.amazonaws.com/files.cnas.org/hero/documents/CNAS_Autonomous-weapons-operational-risk.pdf"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"mitigations",children:"Mitigations"}),"\n",(0,s.jsxs)(n.h3,{id:"centaur-war-teams-see-human-in-the-loop",children:['"Centaur" War Teams (see ',(0,s.jsx)(n.a,{href:"/tags/Human-In-The-Loop",children:"Human In The Loop"}),")"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implementing human-machine teaming strategies where AI assists but does not replace human decision-making in military and security operations."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Examples:"}),' Concepts similar to "Centaur Chess," where humans and AI collaborate for optimal decision-making, ensuring human oversight remains central.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficacy:"})," High \u2013 Human-AI collaboration can enhance decision-making while maintaining ethical constraints."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ease of Implementation:"})," Moderate \u2013 Requires investment in training, AI interpretability, and human-AI interface development."]}),"\n"]}),"\n",(0,s.jsxs)(n.h3,{id:"military-ai-governance-see-global-ai-governance",children:["Military AI Governance (see ",(0,s.jsx)(n.a,{href:"/tags/Global-AI-Governance",children:"Global AI Governance"}),")"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"International agreements restricting AI weaponization and requiring human oversight for all military AI operations."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Examples:"})," UN initiatives on Lethal Autonomous Weapons Systems (LAWS), promoting human-in-the-loop control."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Efficacy:"})," Medium \u2013 Regulations can slow AI weaponization, but enforcement remains a challenge."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ease of Implementation:"})," Low \u2013 Military interests and national security concerns complicate global cooperation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"global-ai-research-and-regulation",children:"Global AI Research and Regulation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:'AI is "Dual Use" - it can be used for military as well as civilian purposes.'}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Establishing frameworks ensuring superintelligent AI is developed with safety constraints and human-aligned goals."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Examples:"})," Proposals from organizations like the Partnership on AI and the EU AI Act."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Efficacy:"})," High \u2013 Regulatory oversight can impose ethical and safety measures to guide AI development."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ease of Implementation:"})," Moderate \u2013 Requires global consensus and enforcement mechanisms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.h3,{id:"kill-switch--override-systems-see-kill-switch",children:["Kill-Switch & Override Systems (See ",(0,s.jsx)(n.a,{href:"/tags/Kill-Switch",children:"Kill Switch"}),")"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Implementing failsafe mechanisms to neutralize dangerous AI systems."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Examples:"})," Research on AI containment methods and fail-safe designs, such as \u201cshutdown problems\u201d in AI alignment."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Efficacy:"})," Medium \u2013 AI systems might learn to bypass or resist shutdown mechanisms."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Ease of Implementation:"})," Low \u2013 Technical challenges in ensuring a reliable and enforceable kill-switch for superintelligent AI."]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(96540);const t={},a=s.createContext(t);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);