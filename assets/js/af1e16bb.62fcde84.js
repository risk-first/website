"use strict";(self.webpackChunkrf_website=self.webpackChunkrf_website||[]).push([[2501],{4353:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var s=t(74848),i=t(28453);const o={title:"Loss Of Human Control",description:"AI systems operating autonomously with minimal human oversight can lead to scenarios where we cannot override or re-align them with human values.",featured:{class:"c",element:'\'<risk class="process" /><description style="text-align: center">Loss of \nHuman Control</description>\'\n'},tags:["AI Threats","Loss Of Human Control"],sidebar_position:3,tweet:"yes",part_of:"AI Threats"},r=void 0,a={id:"ai/Threats/Loss-Of-Human-Control",title:"Loss Of Human Control",description:"AI systems operating autonomously with minimal human oversight can lead to scenarios where we cannot override or re-align them with human values.",source:"@site/docs/ai/Threats/Loss-Of-Human-Control.md",sourceDirName:"ai/Threats",slug:"/ai/Threats/Loss-Of-Human-Control",permalink:"/ai/Threats/Loss-Of-Human-Control",draft:!1,unlisted:!1,editUrl:"https://github.com/risk-first/website/blob/master/docs/ai/Threats/Loss-Of-Human-Control.md",tags:[{inline:!1,label:"AI Threats",permalink:"/tags/AI-Threats"},{inline:!1,label:"Loss Of Human Control",permalink:"/tags/Loss-Of-Human-Control"}],version:"current",sidebarPosition:3,frontMatter:{title:"Loss Of Human Control",description:"AI systems operating autonomously with minimal human oversight can lead to scenarios where we cannot override or re-align them with human values.",featured:{class:"c",element:'\'<risk class="process" /><description style="text-align: center">Loss of \nHuman Control</description>\'\n'},tags:["AI Threats","Loss Of Human Control"],sidebar_position:3,tweet:"yes",part_of:"AI Threats"},sidebar:"tutorialSidebar",previous:{title:"Unintended Cascading Failures",permalink:"/ai/Threats/Unintended-Cascading-Failures"},next:{title:"Loss Of Diversity",permalink:"/ai/Threats/Loss-Of-Diversity"}},l={},c=[{value:"Risk Score: Low",id:"risk-score-low",level:2},{value:"Sources",id:"sources",level:2},{value:"How This Is Already Happening",id:"how-this-is-already-happening",level:2}];function h(e){const n={a:"a",em:"em",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{AIThreatIntro:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("AIThreatIntro",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t,{fm:o}),"\n",(0,s.jsx)(n.h2,{id:"risk-score-low",children:"Risk Score: Low"}),"\n",(0,s.jsx)(n.p,{children:"AI systems may operate autonomously in ways that humans cannot predict or control. If AI entities develop objectives that misalign with human values, we may be unable to stop or correct their actions before harm ensues."}),"\n",(0,s.jsx)(n.p,{children:"##\xa0Description"}),"\n",(0,s.jsx)(n.p,{children:"AI systems that act without robust human oversight can evolve in ways that defy our attempts at control or correction. In the short term, engineers have to wrestle with new approaches to defining acceptable behaviour (see Amodei et al):  even just cleaning an environment is a hard goal to pin down (clean doesn't mean devoid of any furniture, for example).  How do you allow the AI to learn and improve without enabling \"Reward Hacking\", where it finds ways to game the reward function (a la Goodhart's law)."}),"\n",(0,s.jsxs)(n.p,{children:["The problem is that human oversight is ",(0,s.jsx)(n.em,{children:"expensive"}),": we want to have a minimum level of oversight without worrying that things will go wrong."]}),"\n",(0,s.jsx)(n.h2,{id:"sources",children:"Sources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Superintelligence"})," ",(0,s.jsx)(n.a,{href:"https://doi.org/10.1093/acprof:oso/9780199678112.001.0001",children:"Nick Bostrom, 2014"}),': Explores potential pathways by which advanced AI might become misaligned with human objectives, as well as strategies to maintain control, such as the "Paper-clip Maximiser".']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Concrete Problems in AI Safety"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1606.06565",children:"Amodei et al., 2016"}),": Discusses technical challenges of ensuring AI remains controllable, including reward hacking, unintended behaviours, and safe exploration."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Asilomar AI Principles"})," ",(0,s.jsx)(n.a,{href:"https://futureoflife.org/ai-principles/",children:"Future of Life Institute, 2017"}),': Guidelines emphasising the need for human values and safety considerations to guide AI development.  Ensuring that humans remain "in control" is a key principle (but is hard to pin down).']}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"how-this-is-already-happening",children:"How This Is Already Happening"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Flash_crash",children:"Stock Market \u2018Flash Crashes\u2019"}),":"]})," Automated high-frequency trading bots have occasionally triggered large market swings with minimal human intervention, underscoring our reliance on\u2014and vulnerability to\u2014autonomous algorithms."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://mashable.com/article/boeing-737-max-aggressive-risky-ai",children:"Boeing 737 MAX MCAS Issue (2018\u20132019)"}),":"]})," Although not purely an AI system, automated flight software repeatedly overrode pilot inputs, contributing to two tragic crashes\u2014illustrating how over-reliance on opaque automation can lead to disastrous outcomes.  This was caused by systemic failures at Boeing, driven by a cost-cutting culture and short-term focus on shareholder returns."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Healthcare Diagnostic Tools:"})," Systems that recommend or even autonomously administer treatments based on patient data can outpace human doctors\u2019 ability to review every decision, making interventions more difficult if the AI fails.  ",(0,s.jsx)(n.a,{href:"https://ukstories.microsoft.com/features/nhs-grampian-is-working-with-kheiron-medical-technologies-university-of-aberdeen-and-microsoft-to-support-breast-cancer-detection/",children:"NHS Grampian: breast cancer detection."})]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(96540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);