"use strict";(self.webpackChunkrf_website=self.webpackChunkrf_website||[]).push([[7293],{3866:e=>{e.exports=JSON.parse('{"tag":{"label":"AI Practice","permalink":"/tags/AI-Practice","allTagsPath":"/tags","count":14,"items":[{"id":"ai/Practices/AI-As-Judge","title":"AI As Judge","description":"Using the outputs of one (trained) AI to measure the performance of another","permalink":"/ai/Practices/AI-As-Judge"},{"id":"ai/Practices/Ecosystem-Diversity","title":"Ecosystem Diversity","description":"Encouraging the development of multiple, independent AI models instead of relying on a single dominant system.","permalink":"/ai/Practices/Ecosystem-Diversity"},{"id":"ai/Practices/Global-AI-Governance","title":"Global AI Governance","description":"International cooperation is necessary to prevent AI firms from evading national regulations by relocating to jurisdictions with lax oversight.","permalink":"/ai/Practices/Global-AI-Governance"},{"id":"ai/Practices/Human-In-The-Loop","title":"Human In The Loop","description":"Consistent human oversight in critical AI systems.","permalink":"/ai/Practices/Human-In-The-Loop"},{"id":"ai/Practices/Interpretability","title":"Interpretability","description":"Developing tools to analyse AI decision-making processes and detect emergent behaviors before they become risks.","permalink":"/ai/Practices/Interpretability"},{"id":"ai/Practices/Kill-Switch","title":"Kill Switch","description":"Fail-safe systems capable of shutting down or isolating AI processes if they exhibit dangerous behaviours.","permalink":"/ai/Practices/Kill-Switch"},{"id":"practices/Deployment-And-Operations/Monitoring","title":"Monitoring","description":"Continuous observation and tracking of a system, team or person, perhaps with respect to performance, security or availability.","permalink":"/practices/Deployment-And-Operations/Monitoring"},{"id":"ai/Practices/Multi-Stakeholder-Oversight","title":"Multi-Stakeholder Oversight","description":"Governments can implement policies that ensure AI-driven firms remain accountable to human oversight.","permalink":"/ai/Practices/Multi-Stakeholder-Oversight"},{"id":"ai/Practices/National-AI-Regulation","title":"National AI Regulation","description":"Governments can implement policies that ensure AI-driven firms remain accountable to human oversight.","permalink":"/ai/Practices/National-AI-Regulation"},{"id":"ai/Practices/Public-Awareness","title":"Public Awareness","description":"Equip citizens with media literacy skills to spot deepfakes and manipulation attempts.","permalink":"/ai/Practices/Public-Awareness"},{"id":"practices/Deployment-And-Operations/Redundancy","title":"Redundancy","description":"Ensuring backup systems are in place to prevent failure.","permalink":"/practices/Deployment-And-Operations/Redundancy"},{"id":"ai/Practices/Replication-Control","title":"Replication Control","description":"Replication control becomes relevant when an AI system can duplicate itself\u2014or be duplicated\u2014beyond the reach of any central authority.","permalink":"/ai/Practices/Replication-Control"},{"id":"practices/Testing-and-Quality-Assurance/Security-Testing","title":"Security Testing","description":"Ensuring the application is secure by identifying vulnerabilities.","permalink":"/practices/Testing-and-Quality-Assurance/Security-Testing"},{"id":"ai/Practices/Transparency","title":"Transparency","description":"Requiring AI developers to publish model architectures, data sources, generated data and decision-making rationales.","permalink":"/ai/Practices/Transparency"}],"unlisted":false}}')}}]);