"use strict";(self.webpackChunkrf_website=self.webpackChunkrf_website||[]).push([[1694],{28453:(e,t,i)=>{i.d(t,{R:()=>r,x:()=>o});var s=i(96540);const n={},a=s.createContext(n);function r(e){const t=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),s.createElement(a.Provider,{value:t},e.children)}},37679:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"ai/Practices/Kill-Switch","title":"Kill Switch","description":"Fail-safe systems capable of shutting down or isolating AI processes if they exhibit dangerous behaviours.","source":"@site/docs/ai/Practices/Kill-Switch.md","sourceDirName":"ai/Practices","slug":"/ai/Practices/Kill-Switch","permalink":"/ai/Practices/Kill-Switch","draft":false,"unlisted":false,"editUrl":"https://github.com/risk-first/website/blob/master/docs/ai/Practices/Kill-Switch.md","tags":[{"inline":false,"label":"Kill Switch","permalink":"/tags/Kill-Switch"},{"inline":false,"label":"AI Practice","permalink":"/tags/AI-Practice"}],"version":"current","frontMatter":{"title":"Kill Switch","description":"Fail-safe systems capable of shutting down or isolating AI processes if they exhibit dangerous behaviours.","featured":{"class":"c","element":"<action>Kill Switch Mechanism</action>"},"tags":["Kill Switch","AI Practice"],"practice":{"mitigates":[{"tag":"Loss Of Human Control","reason":"An explicit interruption capability can avert catastrophic errors or runaway behaviours"},{"tag":"Synthetic Intelligence With Malicious Intent","reason":"Implementing fail-safe mechanisms to neutralise dangerous AI weapons systems."}]}},"sidebar":"tutorialSidebar","previous":{"title":"Interpretability","permalink":"/ai/Practices/Interpretability"},"next":{"title":"Multi-Stakeholder Oversight","permalink":"/ai/Practices/Multi-Stakeholder-Oversight"}}');var n=i(74848),a=i(28453);const r={title:"Kill Switch",description:"Fail-safe systems capable of shutting down or isolating AI processes if they exhibit dangerous behaviours.",featured:{class:"c",element:"<action>Kill Switch Mechanism</action>"},tags:["Kill Switch","AI Practice"],practice:{mitigates:[{tag:"Loss Of Human Control",reason:"An explicit interruption capability can avert catastrophic errors or runaway behaviours"},{tag:"Synthetic Intelligence With Malicious Intent",reason:"Implementing fail-safe mechanisms to neutralise dangerous AI weapons systems."}]}},o=void 0,c={},l=[{value:"Examples",id:"examples",level:2}];function p(e){const t={h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components},{PracticeIntro:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("PracticeIntro",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i,{details:r}),"\n",(0,n.jsx)(t.h2,{id:"examples",children:"Examples"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Google DeepMind\u2019s \u2018Big Red Button\u2019 concept"})," (2016), proposed as a method to interrupt a reinforcement learning AI without it learning to resist interruption."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Hardware Interrupts in Robotics:"})," Physical or software-based emergency stops that immediately terminate AI operation."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}}}]);