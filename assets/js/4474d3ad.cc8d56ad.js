"use strict";(self.webpackChunkrf_website=self.webpackChunkrf_website||[]).push([[1694],{19814:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var n=i(74848),s=i(28453);const r={title:"Kill Switch",description:"Fail-safe systems capable of shutting down or isolating AI processes if they exhibit dangerous behaviours.",featured:{class:"c",element:"<action>Kill Switch Mechanism</action>"},tags:["Kill Switch","AI Practice"],practice:{mitigates:[{tag:"Loss Of Human Control",reason:"An explicit interruption capability can avert catastrophic errors or runaway behaviours"},{tag:"Synthetic Intelligence With Malicious Intent",reason:"Implementing fail-safe mechanisms to neutralise dangerous AI weapons systems."}]}},a=void 0,o={id:"ai/Practices/Kill-Switch",title:"Kill Switch",description:"Fail-safe systems capable of shutting down or isolating AI processes if they exhibit dangerous behaviours.",source:"@site/docs/ai/Practices/Kill-Switch.md",sourceDirName:"ai/Practices",slug:"/ai/Practices/Kill-Switch",permalink:"/ai/Practices/Kill-Switch",draft:!1,unlisted:!1,editUrl:"https://github.com/risk-first/website/blob/master/docs/ai/Practices/Kill-Switch.md",tags:[{inline:!1,label:"Kill Switch",permalink:"/tags/Kill-Switch"},{inline:!1,label:"AI Practice",permalink:"/tags/AI-Practice"}],version:"current",frontMatter:{title:"Kill Switch",description:"Fail-safe systems capable of shutting down or isolating AI processes if they exhibit dangerous behaviours.",featured:{class:"c",element:"<action>Kill Switch Mechanism</action>"},tags:["Kill Switch","AI Practice"],practice:{mitigates:[{tag:"Loss Of Human Control",reason:"An explicit interruption capability can avert catastrophic errors or runaway behaviours"},{tag:"Synthetic Intelligence With Malicious Intent",reason:"Implementing fail-safe mechanisms to neutralise dangerous AI weapons systems."}]}},sidebar:"tutorialSidebar",previous:{title:"Interpretability",permalink:"/ai/Practices/Interpretability"},next:{title:"Multi-Stakeholder Oversight",permalink:"/ai/Practices/Multi-Stakeholder-Oversight"}},c={},l=[{value:"Examples",id:"examples",level:2}];function p(e){const t={h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{PracticeIntro:i}=t;return i||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("PracticeIntro",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i,{details:r}),"\n",(0,n.jsx)(t.h2,{id:"examples",children:"Examples"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Google DeepMind\u2019s \u2018Big Red Button\u2019 concept"})," (2016), proposed as a method to interrupt a reinforcement learning AI without it learning to resist interruption."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Hardware Interrupts in Robotics:"})," Physical or software-based emergency stops that immediately terminate AI operation."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}},28453:(e,t,i)=>{i.d(t,{R:()=>a,x:()=>o});var n=i(96540);const s={},r=n.createContext(s);function a(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);