"use strict";(self.webpackChunkrf_website=self.webpackChunkrf_website||[]).push([[425],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(96540);const a={},i=o.createContext(a);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:n},e.children)}},69923:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"ai/Practices/Human-In-The-Loop","title":"Human In The Loop","description":"Consistent human oversight in critical AI systems.","source":"@site/docs/ai/Practices/Human-In-The-Loop.md","sourceDirName":"ai/Practices","slug":"/ai/Practices/Human-In-The-Loop","permalink":"/ai/Practices/Human-In-The-Loop","draft":false,"unlisted":false,"editUrl":"https://github.com/risk-first/website/blob/master/docs/ai/Practices/Human-In-The-Loop.md","tags":[{"inline":false,"label":"Human In The Loop","permalink":"/tags/Human-In-The-Loop"},{"inline":false,"label":"AI Practice","permalink":"/tags/AI-Practice"}],"version":"current","frontMatter":{"title":"Human In The Loop","description":"Consistent human oversight in critical AI systems.","featured":{"class":"c","element":"<action>Human In The Loop</action>"},"tags":["Human In The Loop","AI Practice"],"practice":{"mitigates":[{"tag":"Loss Of Human Control","reason":"Maintaining consistent human oversight in critical AI systems, ensuring that final decisions or interventions rest with human operators rather than the AI."},{"tag":"Synthetic Intelligence With Malicious Intent","reason":"See Example of \\"Centaur\\" War Teams"},{"tag":"Unintended Cascading failures","reason":"Human oversight of automated systems can help shortcut cascading failure."}]}},"sidebar":"tutorialSidebar","previous":{"title":"Global AI Governance","permalink":"/ai/Practices/Global-AI-Governance"},"next":{"title":"Interpretability","permalink":"/ai/Practices/Interpretability"}}');var a=t(74848),i=t(28453);const s={title:"Human In The Loop",description:"Consistent human oversight in critical AI systems.",featured:{class:"c",element:"<action>Human In The Loop</action>"},tags:["Human In The Loop","AI Practice"],practice:{mitigates:[{tag:"Loss Of Human Control",reason:"Maintaining consistent human oversight in critical AI systems, ensuring that final decisions or interventions rest with human operators rather than the AI."},{tag:"Synthetic Intelligence With Malicious Intent",reason:'See Example of "Centaur" War Teams'},{tag:"Unintended Cascading failures",reason:"Human oversight of automated systems can help shortcut cascading failure."}]}},r=void 0,c={},l=[{value:"Types Of Human In The Loop",id:"types-of-human-in-the-loop",level:2}];function h(e){const n={a:"a",blockquote:"blockquote",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{PracticeIntro:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("PracticeIntro",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t,{details:s}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Maintaining consistent human oversight in critical AI systems, ensuring that final decisions or interventions rest with human operators rather than the AI."}),"\n",(0,a.jsx)(n.li,{children:"AI may suggest diagnoses or treatments, but a certified professional reviews and confirms before enacting them.  In the above NHS Grampian example, the AI is augmenting human decision making with a third opinion, rather than replacing human judgement altogether (yet)."}),"\n",(0,a.jsx)(n.li,{children:"Some proposals mandate that human operators confirm critical actions (e.g., missile launches), preventing AI from unilaterally making life-or-death decisions.  This might work in scenarios where response time isn't a factor."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"types-of-human-in-the-loop",children:"Types Of Human In The Loop"}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Semi-autonomous operation"}),': machine performs a task and then stops and waits for approval from the human operator before continuing. This control type is often referred to as "human in the loop."']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Supervised autonomous operation"}),", where the machine, once activated, performs a task under the supervision of a human and will continue performing the task unless the human operator intervenes to halt its operation. This control type is often referred to as \u201chuman on the loop.\u201d"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fully autonomous operation"}),", where the machine, once activated, performs a task and the human operator does not have the ability to supervise its operation and intervene in the event of system failure. This control type is often referred to as \u201chuman out of the loop.\u201d"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["From: ",(0,a.jsx)(n.a,{href:"https://s3.us-east-1.amazonaws.com/files.cnas.org/hero/documents/CNAS_Autonomous-weapons-operational-risk.pdf",children:"https://s3.us-east-1.amazonaws.com/files.cnas.org/hero/documents/CNAS_Autonomous-weapons-operational-risk.pdf"})]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);