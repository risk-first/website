% Page setup
\documentclass[12pt]{memoir}
\setstocksize{9.69in}{7.44in}
\settrimmedsize{\stockheight}{\stockwidth}{*}
\setlrmarginsandblock{3.5cm}{2.5cm}{*}
\setulmarginsandblock{2cm}{3cm}{*}
\checkandfixthelayout 
\setheadfoot{\onelineskip}{2\onelineskip}

% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{parskip}    	
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}	

\usepackage{graphicx}					
\usepackage{amssymb}

%SetFonts
\usepackage[T1]{fontenc}
\usepackage{newpxtext,newpxmath}

%Images
\usepackage{graphicx}
% We will generate all images so they have a width .9\maxwidth. This means
% that they will get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
\else\Gin@nat@width\fi}
\makeatother
\let\Oldincludegraphics\includegraphics
\renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.9\maxwidth]{#1}}
\usepackage{rotating}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}
\captionsetup[figure]{labelfont={bf,it},textfont={bf,it}}


% Links
\usepackage[unicode=true]{hyperref}
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={},
            colorlinks=false,
            urlcolor=black,
            linkcolor=black,
            pdfborder={0 0 0}}

% Footers / Page Numbers            (FIX ME)
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
  \renewcommand{\headrulewidth}{0pt}
  \fancyfoot[LE, RO]{\thepage}
  \fancyfoot[C]{\textsl}

% Tables            
\usepackage{longtable,booktabs}
\usepackage[width=.8\textwidth]{caption}
% These lines are needed to make table captions work with longtable:
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{rotating}

% Code Sections
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstnewenvironment{code}{\lstset{basicstyle=\small\ttfamily}}{}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}


%Links as Notes
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
 \renewcommand{\footnotesize}{\fontsize{6.5pt}{8.5pt}\selectfont}

%Sections
\chapterstyle{veelo}
\setlength{\beforechapskip}{20pt}
\setsechook{\hangsecnum}
\setcounter{secnumdepth}{5}

\begin{document}

\frontmatter

\title{Risk-First Software Development: The Menagerie}
\author{Rob Moffat}

\begin{titlingpage}

\hspace{0.05\textwidth}

\centering

{\Huge\bfseries\textsc{Risk-First}}\\[2\baselineskip]

{\Huge\bfseries\textsc{Software Development}}\\[1\baselineskip]

{\Huge\bfseries\textsc{De-Risked }}\\[2\baselineskip]

{\Huge\textit{Volume 1: The Menagerie}}\\[4\baselineskip]

{\Oldincludegraphics[width=0.5\textwidth]{images/R1_logo_grue.png}}\\[4\baselineskip]

{\Huge\textsc{Rob Moffat}}


\end{titlingpage}

\hypertarget{risk-first-the-menagerie}{%
\section{Risk First: The Menagerie}\label{risk-first-the-menagerie}}

By Rob Moffat

Copyright © 2018 Kite9 Ltd.

All rights reserved. No part of this publication may be reproduced,
distributed, or transmitted in any form or by any means, including
photocopying, recording, or other electronic or mechanical methods,
without the prior written permission of the publisher, except in the
case of brief quotations embodied in critical reviews and certain other
noncommercial uses permitted by copyright law. For permission requests,
write to the publisher, addressed ``Attention: Permissions
Coordinator,'' at the address below.

ISBN: tbd.

\hypertarget{credits}{%
\subsection{Credits}\label{credits}}

tbd

Cover Images: Biodiversity Heritage Library. Biologia
Centrali-Americana. Insecta. Rhynchota. Hemiptera-Homoptera. Volume 1
(1881-1905)

Cover Design By P. Moffat (\texttt{peter@petermoffat.com})

Thanks to:

\hypertarget{books-in-the-series}{%
\subsection{Books In The Series}\label{books-in-the-series}}

\begin{itemize}
\tightlist
\item
  \textbf{Risk First: The Menagerie:} Book one of the
  \textbf{Risk-First} series argues the case for viewing \emph{all} of
  the activities on a software project through the lens of
  \emph{managing risk}. It introduces the menagerie of different risks
  you're likely to meet on a software project, naming and classifying
  them so that we can try to understand them better.
\item
  \textbf{Risk First: Tools and Practices:} Book two of the \textbf{Risk
  First} series explores the relationship between software project risks
  and the tools and practices we use to mitigate them. Due for
  publication in 2020.
\end{itemize}

\hypertarget{online}{%
\subsection{Online}\label{online}}

Material for the books is freely available to read, drawn from
\texttt{risk-first.org}.

\hypertarget{published-by}{%
\subsection{Published By}\label{published-by}}

\begin{verbatim}
Kite9 Ltd.
14 Manor Close
Colchester
CO6 4AR
\end{verbatim}

\setcounter{tocdepth}{0}
\tableofcontents

\hypertarget{preface}{%
\chapter{Preface}\label{preface}}

Welcome to Risk-First!

Let's cover some of the big questions up-front: The why, what, who, how
and where of \emph{The Menagerie}.

\hypertarget{why}{%
\section{Why}\label{why}}

\begin{quote}
``Scrum, Waterfall, Lean, Prince2: what do they all have in common?''
\end{quote}

I've started this because, on my career journey, I've noticed that the
way I do things doesn't seem to match up with the way the books
\emph{say} it should be done. And, I found this odd and wanted to
explore it further. Hopefully, you, the reader, will find something of
use in this.

I started with this observation: \emph{Development Teams} put a lot of
faith in methodology. Sometimes, this faith is often so strong it
borders on religion. (Which in itself is a concern.) For some, this is
Prince2. For others, it might be Lean or Agile.

\emph{Developers} put a lot of faith in \emph{particular tools} too.
Some developers are pro-or-anti-Java, others are pro-or-anti-XML. All of
them have their views coloured by their \emph{experiences} (or lack of)
with these tools. Was this because their past projects \emph{succeeded}
or \emph{failed} because of them?

As time went by, I came to see that the choice of methodology, process
or tool was contingent on the problem being solved, and the person
solving the problem. We don't face a shortage of tools in IT, or a
shortage of methodologies, or a shortage of practices. Essentially, that
all the tools and methodologies that the industry had supplied were
there to help \emph{minimize the risk of my project failing}.

This book considers that perspective: that building software is all
about \emph{managing risk}, and that these methodologies are
acknowledgements of this fact, and they differ because they have
\emph{different ideas} about which are the most important \emph{risks to
manage}.

\hypertarget{what-this-is}{%
\section{What This Is}\label{what-this-is}}

Hopefully, after reading this, you'll come away with:

\begin{itemize}
\tightlist
\item
  An appreciation of how risk underpins everything we do as developers,
  whether we want it to or not.
\item
  A framework for evaluating methodologies, tools and practices and
  choosing the right one for the task-at-hand.
\item
  A recontextualization of the software process as being an exercise in
  mitigating different kinds of risk.
\item
  The tools to help you decide when a methodology or tool is
  \emph{letting you down}, and the vocabulary to argue for when it's a
  good idea to deviate from it.
\end{itemize}

This is not intended to be a rigorously scientific work: I don't believe
it's possible to objectively analyze a field like software development
in any meaningful, statistically significant way. (For one, things just
change too fast.)

\begin{quote}
``I have this Pattern''
\end{quote}

Does that diminish it? If you have visited the TVTropes website, you'll
know that it's a set of web-pages describing \emph{common patterns} of
narrative, production, character design etc. to do with fiction. For
example:

\begin{quote}
tbd.
\end{quote}

Is it scientific? No.~Is it correct? Almost certainly. TVTropes is a set
of \emph{empirical patterns} for how stories on TV and other media work.
It's really useful, and a lot of fun. (Warning: it's also incredibly
addictive).

In the same way, tbd, the tbd published a book called ``Design Patterns:
tbd''. Which shows you patterns of \emph{structure} within
Object-Oriented programming:

\begin{quote}
tbd.
\end{quote}

\hypertarget{patterns-for-practitioners}{%
\subsection{Patterns For
Practitioners}\label{patterns-for-practitioners}}

This book aimed to be a set of \emph{useful} patterns which
practitioners could use in their software to achieve certain goals. ``I
have this pattern'' was a phrase used to describe how they had seen a
certain set of constraints before, and how they had solved it in
software.

This book was a set of experts handing down their battle-tested
practices for other developers to use, and, whether you like patterns or
not, knowing them is an important part of being a software developer, as
you will see them used everywhere you go and probably use them yourself.

In the same way, this book aims to be a set of \emph{Patterns for
Software Risk}. Hopefully after reading this book, you will see where
risk hides in software projects, and have a name for it when you see it.

\hypertarget{towards-a-periodic-table}{%
\subsection{Towards a ``Periodic
Table''}\label{towards-a-periodic-table}}

In the latter chapters of ``The Menagerie'' we try to assemble these
risk patterns into a cohesive whole. Projects fail because of risks, and
risks arise from predictable sources.

\hypertarget{what-this-is-not}{%
\subsection{What This is Not}\label{what-this-is-not}}

This is not intended to be a rigorously scientific work: I don't believe
it's possible to objectively analyze a field like software development
in any meaningful, statistically significant way. (For one, things just
change too fast.)

Neither is this site isn't going to be an exhaustive guide of every
possible software development practice and methodology. That would just
be too long and tedious.

Neither is this really a practitioner's guide to using any particular
methodology: If you've come here to learn the best way to do
Retrospectives, then you're in the wrong place. There are plenty of
places you can find that information already. Where possible, this site
will link to or reference concepts on Wikipedia or the wider internet
for further reading on each subject.

\hypertarget{who}{%
\section{Who}\label{who}}

This work is intended to be read by people who work on software
projects, and especially those who are involved in managing software
projects.

If you work collaboratively with other people in a software process, you
should find Risk-First a useful lexicon of terms to help describe the
risks you face.

But here's a warning: This is going to be a depressing book to read. It
is book one of a two-book series, but in \textbf{Book One} you only get
to meet the bad guy.

While \textbf{Book Two} is all about \emph{how to succeed}, This book is
all about how projects \emph{fail}. In it, we're going to try and put
together a framework for understanding the risk of failure, in order
that we can reconstruct our understanding of our activities on a project
based on avoiding it.

So, if you are interested in \emph{avoiding your project failing}, this
is probably going to be useful knowledge.

\hypertarget{for-developers}{%
\subsection{For Developers}\label{for-developers}}

Risk-First is a tool you can deploy to immediately improve your ability
to plan your work.

Frequently, as developers we find software methodologies ``done to us''
from above. Risk-First is a toolkit to help \emph{take apart}
methodologies like Scrum, Lean and Prince2, and understand them.
Methodologies are \emph{bicycles}, rather than \emph{religions}. Rather
than simply \emph{believing}, we can take them apart and see how they
work.

\hypertarget{for-project-managers-and-team-leads}{%
\subsection{For Project Managers and Team
Leads}\label{for-project-managers-and-team-leads}}

All too often, Project Managers don't have a full grasp of the technical
details of their projects. And this is perfectly normal, as the
specialization belongs below them. However, projects fail because risks
materialize, and risks materialize because the devil is in those
details.

This seems like a lost cause, but there is hope: the ways in which risks
materialize on technical projects is the same every time. With
Risk-First we are attempting to name each of these types of risk, which
allows for a dialog with developers about which risks they face, and the
order they should be tackled.

Risk-First allows a project manager to pry open the black box of
development and talk with developers about their work, and how it will
affect the project. It is another tool in the (limited) arsenal of
techniques a project manager can bring to bear on the task of delivering
a successful project.

\hypertarget{how}{%
\section{How}\label{how}}

One of the original proponents of the Agile Manifesto, Kent Beck, begins
his book Extreme Programming by stating:

``It's all about risk'' \textgreater{} Kent Beck

This is a promising start. From there, he introduces his methodology,
Extreme Programming, and explains how you can adopt it in your team, the
features to observe and the characteristics of success and failure.
However, while \emph{Risk} has clearly driven the conception of Extreme
Programming, there is no clear model of software risk underpinning the
work, and the relationship between the practices he espouses and the
risks he is avoiding are hidden.

In this book, we are going to introduce a model of software project
risk. This means that in \textbf{Book Two} (Risk-First: Tools and
Practices), we can properly analyse Extreme Programming (and Scrum,
Waterfall, Lean and all the others) and \emph{understand} what drives
them. Since they are designed to deliver successful software projects,
they must be about mitigate risks, and we will uncover \emph{exactly
which risks are mitigated} and \emph{how they do it}.

\hypertarget{where}{%
\section{Where}\label{where}}

All of the material for this book is available Open Source on
\href{https://github.com}{github.com}, and at the
\href{https://risk-first.org}{risk-first.org} website. Please visit,
your feedback is appreciated.

There is no compulsion to buy a print or digital version of the book,
but we'd really appreciate the support. So, if you've read this and
enjoyed it, how about buying a copy for someone else to read?

\hypertarget{a-note-on-references}{%
\subsection{A Note on References}\label{a-note-on-references}}

Where possible, references are to the
\href{https://wikipedia.org}{Wikipedia} website. Wikipedia is not
perfect. There is a case for linking to the original articles and
papers, but by using Wikipedia references are free and easy for everyone
to access, and hopefully will exist for a long time into the future.

On to The Executive Summary

\hypertarget{executive-summary}{%
\chapter{Executive Summary}\label{executive-summary}}

\hypertarget{there-are-lots-of-ways-of-running-software-projects}{%
\section{1. There are Lots of Ways of Running Software
Projects}\label{there-are-lots-of-ways-of-running-software-projects}}

There are lots of different ways to look at a project. For example,
metrics such as ``number of open tickets'', ``story points'', ``code
coverage'' or ``release cadence'' give us a numerical feel for how
things are going and what needs to happen next. We also judge the health
of projects by the practices used on them - Continuous Integration, Unit
Testing or Pair Programming, for example.

Software methodologies, then, are collections of tools and practices:
``Agile'', ``Waterfall'', ``Lean'' or ``Phased Delivery'' (for example)
all suggest different approaches to running a project, and are
opinionated about the way they think projects should be done and the
tools that should be used.

None of these is necessarily more ``right'' than another- they are
suitable on different projects at different times.

A key question then is: \textbf{how do we select the right tools for the
job?}

\hypertarget{we-can-look-at-projects-in-terms-of-risks}{%
\section{2. We can Look at Projects in Terms of
Risks}\label{we-can-look-at-projects-in-terms-of-risks}}

One way to examine a project in-flight is by looking at the risks it
faces.

Commonly, tools such as RAID logs and RAG status reporting are used.
These techniques should be familiar to project managers and developers
everywhere.

However, the Risk-First view is that we can go much further: that each
item of work being done on the project is mitigating a particular risk.
Risk isn't something that just appears in a report, it actually drives
\emph{everything we do}.

For example:

\begin{itemize}
\tightlist
\item
  A story about improving the user login screen can be seen as reducing
  \emph{the risk of users not signing up}.
\item
  A task about improving the health indicators could be seen as
  mitigating \emph{the risk of the application failing and no-one
  reacting to it}.
\item
  Even a task as basic as implementing a new function in the application
  is mitigating \emph{the risk that users are dissatisfied and go
  elsewhere}.
\end{itemize}

\textbf{One assertion of Risk-First therefore, is that every action you
take on a project is to mitigate some risk.}

\hypertarget{we-can-break-down-risks-on-a-project-methodically}{%
\section{3. We Can Break Down Risks on a Project
Methodically}\label{we-can-break-down-risks-on-a-project-methodically}}

Although risk is usually complicated and messy, other industries have
found value in breaking down the types of risks that affect them and
addressing them individually.

For example:

\begin{itemize}
\tightlist
\item
  In manufacturing, \emph{tolerances} allow for calculating the
  likelihood of defects in production.
\item
  In finance, reserves are commonly set aside for the risks of
  stock-market crashes, and teams are structured around monitoring these
  different risks.
\item
  The insurance industry is founded on identifying particular risks and
  providing financial safety-nets for when they occur, such as death,
  injury, accident and so on.
\end{itemize}

Software risks are difficult to quantify, and mostly, the effort
involved in doing so \emph{exactly} would outweigh the benefit.
Nevertheless, there is value in spending time building
\emph{classifications of risk for software}. That's what Risk-First
does: describes the set of \emph{risk patterns} we see every day on
software projects.

With this in place, we can:

\begin{itemize}
\tightlist
\item
  Talk about the types of risks we face on our projects, using an
  appropriate language.
\item
  Expose Hidden Risks that we hadn't considered before.
\item
  Weigh the risks against each other, and decide which order to tackle
  them.
\end{itemize}

\hypertarget{we-can-analyse-tools-and-techniques-in-terms-of-how-they-mitigate-risk}{%
\section{4. We Can Analyse Tools and Techniques in Terms of how they
Mitigate
Risk}\label{we-can-analyse-tools-and-techniques-in-terms-of-how-they-mitigate-risk}}

If we accept the assertion above that \emph{all} the actions we take on
a project are about mitigating risks, then it stands to reason that the
tools and techniques available to us on a project are there for
mitigating different types of risks.

For example:

\begin{itemize}
\tightlist
\item
  If we do a Code Review, we are partly trying to mitigate the risks of
  bugs slipping through into production, and also mitigate the Key-Man
  Risk of knowledge not being widely-enough shared.
\item
  If we write Unit Tests, we're also mitigating the risk of bugs going
  to production, but we're also mitigating against future changes
  breaking our existing functionality.
\item
  If we enter into a contract with a supplier, we are mitigating the
  risk of the supplier vanishing and leaving us exposed. With the
  contract in place, we have legal recourse against this risk.
\end{itemize}

\textbf{Different tools are appropriate for mitigating different types
of risks.}

\hypertarget{different-methodologies-for-different-risk-profiles}{%
\section{5. Different Methodologies for Different Risk
Profiles}\label{different-methodologies-for-different-risk-profiles}}

In the same way that our tools and techniques are appropriate to dealing
with different risks, the same is true of the methodologies we use on
our projects. We can use a Risk-First approach to examine the different
methodologies, and see which risks they address.

For example:

\begin{itemize}
\tightlist
\item
  \textbf{Agile} methodologies prioritise mitigating the risk that
  requirements capture is complicated, error-prone and that requirements
  change easily.
\item
  \textbf{Waterfall} takes the view that coding effort is an expensive
  risk, and that we should build plans up-front to avoid it.
\item
  \textbf{Lean} takes the view that risk lies in incomplete work and
  wasted work, and aims to minimize that.
\end{itemize}

Although many developers have a methodology-of-choice, the argument here
is that there are tradeoffs with all of these choices. Methodologies are
like \emph{bicycles}, rather than \emph{religions}. Rather than simply
\emph{believing}, we can take them apart and see how they work.

\textbf{We can place methodologies within a framework, and show how
choice of methodology is contingent on the risks faced.}

\hypertarget{driving-development-with-a-risk-first-perspective}{%
\section{6. Driving Development With a Risk-First
Perspective}\label{driving-development-with-a-risk-first-perspective}}

We have described a model of risk within software projects, looking
something like this:

\begin{figure}
\centering
\includegraphics{images/generated/pattern_language-400dpi.png}
\caption{Methdologies, Risks, Practices}
\end{figure}

How do we take this further?

The first idea we explore is that of the Risk Landscape: Although the
software team can't remove risk from their project, they can take
actions that move them to a place in the Risk Landscape where the risks
on the project are more favourable than where they started.

From there, we examine basic risk archetypes you will encounter on the
software project, to build up a Taxonomy of Software Risk, and look at
which specific tools you can use to mitigate each kind of risk.

Then, we look at different software practices, and how they mitigate
various risks. Beyond this we examine the question: \emph{how can a
Risk-First approach inform the use of this technique?}

For example:

\begin{itemize}
\tightlist
\item
  If we are introducing a \textbf{Sign-Off} in our process, we have to
  balance the risks it \emph{mitigates} (coordination of effort, quality
  control, information sharing) with the risks it \emph{introduces}
  (delays and process bottlenecks).
\item
  If we have \textbf{Redundant Systems}, this mitigates the risk of a
  \emph{single point of failure}, but introduces risks around
  \emph{synchronizing data} and \emph{communication} between the
  systems.
\item
  If we introduce \textbf{Process}, this may make it easier to
  \emph{coordinate as a team} and \emph{measure performance} but may
  lead to bureaucracy, focusing on the wrong goals or over-rigid
  interfaces to those processes.
\end{itemize}

Risk-First aims to provide a framework in which we can \emph{analyse
these choices} and weigh up \emph{accepting} versus \emph{mitigating}
risks.

\textbf{Still interested? Then dive into reading the introduction.}

\mainmatter
\part{Introduction}

\part{Risk}

\hypertarget{complexity-risk}{%
\chapter{Complexity Risk}\label{complexity-risk}}

Complexity Risk are the risks to your project due to its underlying
``complexity''. Over the next few chapters, we'll break down exactly
what we mean by complexity, looking at Dependency Risk and Boundary Risk
as two particular sub-types of Complexity Risk. However, in this
chapter, we're going to be specifically focusing on \emph{code you
write}: the size of your code-base, the number of modules, the
interconnectedness of the modules and how well-factored the code is.

\begin{figure}
\centering
\includegraphics{images/generated/complexity-risk-400dpi.png}
\caption{Complexity Risks}
\end{figure}

You could think of this chapter, then, as \textbf{Codebase Risk}: We'll
look at three separate measures of codebase complexity and talk about
Technical Debt, and look at places in which \textbf{Codebase Risk} is at
it's greatest.

\hypertarget{kolmogorov-complexity}{%
\section{Kolmogorov Complexity}\label{kolmogorov-complexity}}

The standard Computer-Science definition of complexity, is
\href{https://en.wikipedia.org/wiki/Kolmogorov_complexity}{Kolmogorov
Complexity}. This is:

\begin{quote}
``\ldots{}is the length of the shortest computer program (in a
predetermined programming language) that produces the object as
output.'' - Kolmogorov Complexity, Wikipedia
\end{quote}

This is a fairly handy definition for us, as it means that to in writing
software to solve a problem, there is a lower bound on the size of the
software we write. In practice, this is pretty much impossible to
quantify. But that doesn't really matter: the techniques for
\emph{moving in that direction} are all that we are interested in, and
this basically amounts to compression.

Let's say we wanted to write a javascript program to output this string:

\begin{verbatim}
abcdabcdabcdabcdabcdabcdabcdabcdabcdabcd
\end{verbatim}

We might choose this representation:

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{function} \AttributeTok{out}\NormalTok{() }\OperatorTok{\{}\NormalTok{                                 (}\DecValTok{7}\NormalTok{ symbols)}
    \ControlFlowTok{return} \StringTok{"abcdabcdabcdabcdabcdabcdabcdabcdabcdabcd"}\NormalTok{   (}\DecValTok{45}\NormalTok{)}
\OperatorTok{\}}\NormalTok{                                                       (}\DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\ldots{} which contains \textbf{53} symbols, if you count
\texttt{function}, \texttt{out} and \texttt{return} as one symbol each.

But, if we write it like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{const}\NormalTok{ ABCD}\OperatorTok{=}\StringTok{"ABCD"}\OperatorTok{;}\NormalTok{                                           (}\DecValTok{11}\NormalTok{ symbols)}

\KeywordTok{function} \AttributeTok{out}\NormalTok{() }\OperatorTok{\{}\NormalTok{                                             (}\DecValTok{7}\NormalTok{ symbols)}
    \ControlFlowTok{return}\NormalTok{ ABCD}\OperatorTok{+}\NormalTok{ABCD}\OperatorTok{+}\NormalTok{ABCD}\OperatorTok{+}\NormalTok{ABCD}\OperatorTok{+}\NormalTok{ABCD}\OperatorTok{+}\NormalTok{ABCD}\OperatorTok{+}\NormalTok{ABCD}\OperatorTok{+}\NormalTok{ABCD}\OperatorTok{+}\NormalTok{ABCD}\OperatorTok{+}\AttributeTok{ABCD}\NormalTok{ (}\DecValTok{21}\NormalTok{ symbols)}
\OperatorTok{\}}\NormalTok{                                                            (}\DecValTok{1}\NormalTok{ symbol)}
\end{Highlighting}
\end{Shaded}

With this version, we now have \textbf{40} symbols. And with this
version:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{const}\NormalTok{ ABCD}\OperatorTok{=}\StringTok{"ABCD"}\OperatorTok{;}\NormalTok{                                           (}\DecValTok{11}\NormalTok{ symbols)}

\KeywordTok{function} \AttributeTok{out}\NormalTok{() }\OperatorTok{\{}\NormalTok{                                             (}\DecValTok{7}\NormalTok{ symbols)}
    \ControlFlowTok{return} \VariableTok{ABCD}\NormalTok{.}\AttributeTok{repeat}\NormalTok{(}\DecValTok{10}\NormalTok{)                                   (}\DecValTok{7}\NormalTok{ symbols)}
\OperatorTok{\}}\NormalTok{                                                            (}\DecValTok{1}\NormalTok{ symbol)}
\end{Highlighting}
\end{Shaded}

\ldots{} we have \textbf{26} symbols.

\hypertarget{abstraction}{%
\subsection{Abstraction}\label{abstraction}}

What's happening here is that we're \emph{exploiting a pattern}: we
noticed that \texttt{ABCD} occurs several times, so we defined it a
single time and then used it over and over, like a stamp. Separating the
\emph{definition} of something from the \emph{use} of something as we've
done here is called ``abstraction''. We're going to come across it over
and over again in this part of the book, and not just in terms of
computer programs.

By applying techniques such as Abstraction, we can improve in the
direction of the Kolmogorov limit. And, by allowing ourselves to say
that \emph{symbols} (like \texttt{out} and \texttt{ABCD}) are worth one
complexity point, we've allowed that we can be descriptive in our
\texttt{function} name and \texttt{const}. Naming things is an important
part of abstraction, because to use something, you have to be able to
refer to it.

\hypertarget{trade-off}{%
\subsection{Trade-Off}\label{trade-off}}

But we could go further down into
\href{https://en.wikipedia.org/wiki/Code_golf}{Code Golf} territory.
This javascript program plays
\href{https://en.wikipedia.org/wiki/Fizz_buzz}{FizzBuzz} up to 100, but
is less readable than you might hope:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i}\OperatorTok{=}\DecValTok{0}\OperatorTok{;}\NormalTok{i}\OperatorTok{<}\DecValTok{100}\OperatorTok{;}\NormalTok{)}\VariableTok{document}\NormalTok{.}\AttributeTok{write}\NormalTok{(((}\OperatorTok{++}\NormalTok{i}\OperatorTok{%}\DecValTok{3}\OperatorTok{?}\StringTok{''}\NormalTok{:}\StringTok{'Fizz'}\NormalTok{)}\OperatorTok{+}
\NormalTok{(i}\OperatorTok{%}\DecValTok{5}\OperatorTok{?}\StringTok{''}\NormalTok{:}\StringTok{'Buzz'}\NormalTok{)}\OperatorTok{||}\NormalTok{i)}\OperatorTok{+}\StringTok{"<br>"}\NormalTok{)                                  (}\DecValTok{66}\NormalTok{ symbols)}
\end{Highlighting}
\end{Shaded}

So there is at some point a trade-off to be made between Complexity Risk
and Communication Risk. This is a topic we'll address more in that
chapter. But for now, it should be said that Communication Risk is about
\emph{misunderstanding}: The more complex a piece of software is, the
more difficulty users will have understanding it, and the more
difficulty developers will have changing it.

\hypertarget{connectivity}{%
\section{Connectivity}\label{connectivity}}

A second, useful measure of complexity comes from graph theory, and that
is the connectivity of a graph:

\begin{quote}
``\ldots{}the minimum number of elements (nodes or edges) that need to
be removed to disconnect the remaining nodes from each other'' -
\href{https://en.wikipedia.org/wiki/Connectivity_(graph_theory)}{Connectivity,
\emph{Wikipedia}}
\end{quote}

To see this in action, have a look at the below graph:

\begin{figure}
\centering
\includegraphics{images/generated/connectivity_1-400dpi.png}
\caption{Graph 1}
\end{figure}

It has 10 vertices, labelled \textbf{a} to \textbf{j}, and it has 15
edges (or links) connecting the vertices together. If any single edge
were removed from this diagram, the 10 vertices would still be linked
together. Because of this, we can say that the graph is
\emph{2-connected}. That is, to disconnect any single vertex, you'd have
to remove \emph{at least} two edges.

As a slight aside, let's consider the \textbf{Kolmogorov Complexity} of
this graph, by inventing a mini-language to describe graphs. It could
look something like this:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{<}\NormalTok{item}\OperatorTok{>} \OperatorTok{:}\NormalTok{ [}\OperatorTok{<}\NormalTok{item}\OperatorTok{>,}\NormalTok{]}\OperatorTok{*} \OperatorTok{<}\NormalTok{item}\OperatorTok{>}\NormalTok{    # Indicates that the item before the colon}
\NormalTok{                              # has a connection to all the items after the }\VariableTok{colon}\NormalTok{.}

\NormalTok{a}\OperatorTok{:}\NormalTok{ b}\OperatorTok{,}\NormalTok{c}\OperatorTok{,}\NormalTok{d}
\NormalTok{b}\OperatorTok{:}\NormalTok{ c}\OperatorTok{,}\NormalTok{f}\OperatorTok{,}\NormalTok{e}
\NormalTok{c}\OperatorTok{:}\NormalTok{ f}\OperatorTok{,}\NormalTok{d}
\NormalTok{d}\OperatorTok{:}\NormalTok{ j}
\NormalTok{e}\OperatorTok{:}\NormalTok{ h}\OperatorTok{,}\NormalTok{j}
\NormalTok{f}\OperatorTok{:}\NormalTok{ h}
\NormalTok{g}\OperatorTok{:}\NormalTok{ j}
\NormalTok{h}\OperatorTok{:}\NormalTok{ i}
\NormalTok{i}\OperatorTok{:} \AttributeTok{j}\NormalTok{                                                         (}\DecValTok{39}\NormalTok{ symbols)}
\end{Highlighting}
\end{Shaded}

Let's remove some of those extra links:

\begin{figure}
\centering
\includegraphics{images/generated/connectivity_2-400dpi.png}
\caption{Graph 2}
\end{figure}

In this graph, I've removed 6 of the edges. Now, we're in a situation
where if any single edge is removed, the graph becomes
\emph{unconnected}. That is, it's broken into distinct chunks. So, it's
\emph{1-connected}.

The second graph is clearly simpler than the first. And, we can show
this by looking at the \textbf{Kolgomorov Complexity} in our little
language:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a}\OperatorTok{:}\NormalTok{ d}\OperatorTok{,}\NormalTok{g}
\NormalTok{b}\OperatorTok{:}\NormalTok{ f}
\NormalTok{c}\OperatorTok{:}\NormalTok{ d}\OperatorTok{,}\NormalTok{f}
\NormalTok{d}\OperatorTok{:}\NormalTok{ j}
\NormalTok{f}\OperatorTok{:}\NormalTok{ h}
\NormalTok{e}\OperatorTok{:}\NormalTok{ h}
\NormalTok{h}\OperatorTok{:} \AttributeTok{i}\NormalTok{                                                         (}\DecValTok{25}\NormalTok{ symbols)}
\end{Highlighting}
\end{Shaded}

\textbf{Connectivity} is also \textbf{Complexity}. Heavily connected
programs/graphs are much harder to work with than less-connected ones.
Even \emph{laying out} the first graph sensibly is a harder task than
the second (the second is a doddle). But the reason programs with
greater connectivity are harder to work with is that changing one module
potentially impacts many others.

\hypertarget{hierarchies-and-modularization}{%
\section{Hierarchies and
Modularization}\label{hierarchies-and-modularization}}

In the second, simplified graph, I've arranged it as a hierarchy, which
I can do now that it's only 1-connected. For 10 vertices, we need 9
edges to connect everything up. It's always:

\begin{verbatim}
  edges = vertices - 1
\end{verbatim}

Note that I could pick any hierarchy here: I don't have to start at
\textbf{c} (although it has the nice property that it has two roughly
even sub-trees attached to it).

How does this help us? Imagine if \textbf{a} - \textbf{j} were modules
of a software system, and the edges of the graph showed communications
between the different sub-systems. In the first graph, we're in a worse
position: who's in charge? What deals with what? Can I isolate a
component and change it safely? What happens if one component
disappears? But, in the second graph, it's easier to reason about,
because of the reduced number of connections and the new heirarchy of
organisation.

On the downside, perhaps our messages have farther to go now: in the
original \textbf{i} could send a message straight to \textbf{j}, but now
we have to go all the way via \textbf{c}. But this is the basis of
\href{https://en.wikipedia.org/wiki/Modular_programming}{Modularization}
and \href{https://en.wikipedia.org/wiki/Hierarchy}{Hierarchy}.

As a tool to battle complexity, we don't just see this in software, but
everywhere in our lives. Society, business, nature and even our bodies:

\begin{itemize}
\tightlist
\item
  \textbf{Organelles} - such as
  \href{https://en.wikipedia.org/wiki/Mitochondrion}{Mitochondria}.
\item
  \textbf{Cells} - such as blood cells, nerve cells, skin cells in the
  \href{https://en.wikipedia.org/wiki/List_of_distinct_cell_types_in_the_adult_human_body}{Human
  Body}.
\item
  \textbf{Organs} - like hearts livers, brains etc.
\item
  \textbf{Organisms} - like you and me.
\end{itemize}

The great complexity-reducing mechanism of modularization is that
\emph{you only have to consider your local environment}. Elements of the
program that are ``far away'' in the hierarchy can be relied on not to
affect you. This is somewhat akin to the \textbf{Principal Of Locality}:

\begin{quote}
``Spatial locality refers to the use of data elements within relatively
close storage locations.'' -
\href{https://en.wikipedia.org/wiki/Locality_of_reference}{Locality Of
Reference, \emph{Wikipedia}}
\end{quote}

\hypertarget{cyclomatic-complexity}{%
\section{Cyclomatic Complexity}\label{cyclomatic-complexity}}

A variation on this graph connectivity metric is our third measure of
complexity,
\href{https://en.wikipedia.org/wiki/Cyclomatic_complexity}{Cyclomatic
Complexity}. This is:

\begin{verbatim}
Cyclomatic Complexity = edges − vertices + 2P,
\end{verbatim}

Where \textbf{P} is the number of \textbf{Connected Components}
(i.e.~distinct parts of the graph that aren't connected to one another
by any edges).

So, our first graph had a \textbf{Cyclomatic Complexity} of 7.
\texttt{(15\ -\ 10\ +\ 2)}, while our second was 1.
\texttt{(9\ -\ 10\ +\ 2)}.

Cyclomatic complexity is all about the number of different routes
through the program. The more branches a program has, the greater it's
cyclomatic complexity. Hence, this is a useful metric in Testing and
Code Coverage: the more branches you have, the more tests you'll need to
exercise them all.

\hypertarget{more-abstraction}{%
\section{More Abstraction}\label{more-abstraction}}

Although we ended up with our second graph having a \textbf{Cyclomatic
Complexity} of 1 (the minimum), we can go further through abstraction,
because this representation isn't minimal from a \textbf{Kolmogorov
Complexity} point-of-view. For example, we might observe that there are
further similarities in the graph that we can ``draw out'':

\begin{figure}
\centering
\includegraphics{images/generated/connectivity_3-400dpi.png}
\caption{Complexity 3}
\end{figure}

Here, we've spotted that the structure of subgraphs \textbf{P1} and
\textbf{P2} are the same: we can have the same functions there to
assemble those. Noticing and exploiting patterns of repetition is one of
the fundamental tools we have in the fight against Complexity Risk.

So, we've looked at some measures of software structure complexity, in
order that we can say ``this is more complex than this''. However, we've
not really said why complexity entails Risk. So let's address that now
by looking at two analogies, Mass and Technical Debt.

\hypertarget{complexity-as-mass}{%
\section{Complexity As Mass}\label{complexity-as-mass}}

The first way to look at complexity is as \textbf{Mass} or
\textbf{Inertia} : a software project with more complexity has greater
\textbf{Inertia} or \textbf{Mass} than one with less complexity.

Newton's Second Law states:

\begin{quote}
``F = \emph{m}\textbf{a}, ( Force = Mass x Acceleration )'' -
\href{https://en.wikipedia.org/wiki/Newton\%27s_laws_of_motion}{Netwon's
Laws Of Motion, \emph{Wikipedia}}
\end{quote}

That is, in order to move your project \emph{somewhere new}, and make it
do new things, you need to give it a push, and the more \textbf{Mass} it
has, the more \textbf{Force} you'll need to move (accelerate) it.

\textbf{Inertia} and \textbf{Mass} are equivalent concepts in physics:

\begin{quote}
``mass is the quantitative or numerical measure of a body's inertia,
that is of its resistance to being accelerated''. -
\href{https://en.wikipedia.org/wiki/Inertia\#Mass_and_inertia}{Inertia,
\emph{Wikipedia}}
\end{quote}

You could stop here and say that the more lines of code a project
contains, the higher it's mass. And, that makes sense, because in order
to get it to do something new, you're likely to need to change more
lines.

But there is actually some underlying sense in which \emph{this is
real}, as discussed in this
\href{https://www.youtube.com/user/1veritasium}{Veritasium} video. To
paraphrase:

\begin{quote}
``Most of your mass you owe due to E=mc², you owe to the fact that your
mass is packed with energy, because of the \textbf{interactions} between
these quarks and gluon fluctuations in the gluon field\ldots{} what we
think of as ordinarily empty space\ldots{} that turns out to be the
thing that gives us most of our mass.'' -
\href{https://www.youtube.com/watch?annotation_id=annotation_3771848421\&feature=iv\&src_vid=Xo232kyTsO0\&v=Ztc6QPNUqls}{Your
Mass is NOT From the Higgs Boson, \emph{Veritasium}}
\end{quote}

I'm not an expert in physics, \emph{at all}, and so there is every
chance that I am pushing this analogy too hard. But, substituting quarks
and gluons for pieces of software we can (in a very handwaving-y way)
say that more complex software has more \textbf{interactions} going on,
and therefore has more mass than simple software.

The reason I am labouring this analogy is to try and make the point that
Complexity Risk is really fundamental:

\begin{itemize}
\tightlist
\item
  Feature Risk: like \textbf{money}.
\item
  Schedule Risk: like \textbf{time}.
\item
  Complexity Risk: like \textbf{mass}.
\end{itemize}

At a basic level, Complexity Risk heavily impacts on Schedule Risk: more
complexity means you need more force to get things done, which takes
longer.

\hypertarget{technical-debt}{%
\section{Technical Debt}\label{technical-debt}}

The most common way we talk about unnecessary complexity in software is
as Technical Debt:

\begin{quote}
``Shipping first time code is like going into debt. A little debt speeds
development so long as it is paid back promptly with a rewrite\ldots{}
The danger occurs when the debt is not repaid. Every minute spent on
not-quite-right code counts as interest on that debt. Entire engineering
organizations can be brought to a stand-still under the debt load of an
unconsolidated implementation, object-oriented or otherwise.'' --
\href{https://en.wikipedia.org/wiki/Technical_debt}{Ward Cunningham,
1992}
\end{quote}

Building a perfect first-time solution is a waste, because perfection
takes a long time. You're taking on more attendant Schedule Risk than
necessary and Meeting Reality more slowly than you could.

A quick-and-dirty, over-complex implementation mitigates the same
Feature Risk and allows you to Meet Reality faster (see Prototyping).

But, having mitigated the Feature Risk, you are now carrying more
Complexity Risk than you necessarily need, and it's time to think about
how to Refactor the software to reduce this risk again.

\hypertarget{kitchen-analogy}{%
\section{Kitchen Analogy}\label{kitchen-analogy}}

It's often hard to make the case for minimizing Technical Debt: it often
feels that there are more important priorities, especially when
technical debt can be ``swept under the carpet'' and forgotten about
until later. (See Discounting The Future.)

One helpful analogy I have found is to imagine your code-base is a
kitchen. After preparing a meal (i.e.~delivering the first
implementation), \emph{you need to tidy up the kitchen}. This is just
something everyone does as a matter of \emph{basic sanitation}.

Now of course, you could carry on with the messy kitchen. When tomorrow
comes and you need to make another meal, you find yourself needing to
wash up saucepans as you go, or working around the mess by using
different surfaces to chop on.

It's not long before someone comes down with food poisoning.

We wouldn't tolerate this behaviour in a restaurant kitchen, so why put
up with it in a software project?

\hypertarget{feature-creep}{%
\section{Feature Creep}\label{feature-creep}}

In Brooks' essay ``No Silver Bullet - Essence and Accident in Software
Engineering'', a distinction is made between:

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{Essence}: \emph{the difficulties inherent in the nature of the
  software.}
\item
  \textbf{Accident}: \emph{those difficulties that attend its production
  but are not inherent.}

  \begin{itemize}
  \tightlist
  \item
    \href{https://en.wikipedia.org/wiki/No_Silver_Bullet}{Fred Brooks,
    \emph{No Silver Bullet}}
  \end{itemize}
\end{itemize}
\end{quote}

The problem with this definition is that we are accepting features of
our software as \emph{essential}.

The \textbf{Risk-First} approach is that if you want to mitigate some
Feature Risk then you have to pick up Complexity Risk as a result. But,
that's a \emph{choice you get to make}.

Therefore, \href{https://en.wikipedia.org/wiki/Feature_creep}{Feature
Creep} (or
\href{https://en.wikipedia.org/wiki/Gold_plating_(software_engineering)}{Gold
Plating}) is a failure to observe this basic equation: instead of
considering this trade off, you're building every feature possible. This
has an impact on Complexity Risk, which in turn impacts Communication
Risk and also Schedule Risk.

Sometimes, feature-creep happens because either managers feel they need
to keep their staff busy, or the staff decide on their own that they
need to keep themselves busy. But now, we can see that basically this
boils down to bad risk management.

\begin{quote}
``Perfection is Achieved Not When There Is Nothing More to Add, But When
There Is Nothing Left to Take Away'' - Antoine de Saint-Exupery
\end{quote}

\hypertarget{dead-end-risk}{%
\section{Dead-End Risk}\label{dead-end-risk}}

\begin{figure}
\centering
\includegraphics{images/generated/dead-end-risk-400dpi.png}
\caption{Dead-End Risk}
\end{figure}

Dead-End Risk is where you build functionality that you \emph{think} is
useful, only to find out later that actually, it was a dead-end, and is
superceded by something else.

For example, let's say that the Accounting sub-system needed password
protection (so you built this). Then the team realised that you needed a
way to \emph{change the password} (so you built that). Then, that you
needed to have more than one user of the Accounting system so they would
all need passwords (ok, fine).

Finally, the team realises that actually logging-in would be something
that all the sub-systems would need, and that it had already been
implemented more thoroughly by the Approvals sub-system.

At this point, you realise you're in a \textbf{Dead End}:

\begin{itemize}
\tightlist
\item
  \textbf{Option 1}: You carry on making minor incremental improvements
  to the accounting password system (carrying the extra Complexity Risk
  of the duplicated functionality).
\item
  \textbf{Option 2}: You rip out the accounting password system, and
  merge in the Approvals system, surfacing new, hidden Complexity Risk
  in the process, due to the difficulty in migrating users from the old
  to new way of working.
\item
  \textbf{Option 3}: You start again, trying to take into account both
  sets of requirements at the same time, again, possibly surfacing new
  hidden Complexity Risk due to the combined approach.
\end{itemize}

Sometimes, the path from your starting point to your goal on the Risk
Landscape will take you to dead ends: places where the only way towards
your destination is to lose something, and do it again another way.

This is because you surface new Hidden Risk along the way. And the
source of a lot of this hidden risk will be unexpected Complexity Risk
in the solutions you choose. This happens a lot.

\hypertarget{source-control}{%
\subsection{Source Control}\label{source-control}}

\href{https://en.wikipedia.org/wiki/Version_control}{Version Control
Systems} like \href{https://en.wikipedia.org/wiki/Git}{Git} are a useful
mitigation of Dead-End Risk, because it means you can \emph{go back} to
the point where you made the bad decision and go a different way.
Additionally, they provide you with backups against the often
inadvertent Dead-End Risk of someone wiping the hard-disk.

\hypertarget{the-re-write}{%
\subsection{The Re-Write}\label{the-re-write}}

\textbf{Option 3}, Rewriting code or a whole project can seem like a way
to mitigate Complexity Risk, but it usually doesn't work out too well.
As Joel Spolsky says:

\begin{quote}
There's a subtle reason that programmers always want to throw away the
code and start over. The reason is that they think the old code is a
mess. And here is the interesting observation: they are probably wrong.
The reason that they think the old code is a mess is because of a
cardinal, fundamental law of programming: \emph{It's harder to read code
than to write it.} -
\href{https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/}{Things
You Should Never Do, Part 1, \emph{Joel Spolsky}}
\end{quote}

The problem that Joel is outlining here is that the developer mistakes
hard-to-understand code for unnecessary Complexity Risk. Also, perhaps
there is Agency Risk because the developer is doing something that is
more useful to him than the project. We're going to return to this
problem in again Communication Risk.

\hypertarget{where-complexity-hides}{%
\section{Where Complexity Hides}\label{where-complexity-hides}}

Complexity isn't spread evenly within a software project. Some problems,
some areas, have more than their fair share of issues. We're going to
cover a few of these now, but be warned, this is not a complete list by
any means:

\begin{itemize}
\tightlist
\item
  Memory Management
\item
  Protocols / Types
\item
  Algorithmic (Space and Time) Complexity
\item
  Concurrency / Mutability
\item
  Networks / Security
\end{itemize}

\hypertarget{memory-management}{%
\subsection{Memory Management}\label{memory-management}}

Memory Management is another place where Complexity Risk hides:

\begin{quote}
``Memory leaks are a common error in programming, especially when using
languages that have no built in automatic garbage collection, such as C
and C++.'' - \href{https://en.wikipedia.org/wiki/Memory_leak}{Memory
Leak, \emph{Wikipedia}}
\end{quote}

\href{https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)}{Garbage
Collectors} (as found in Javascript or Java) offer you the deal that
they will mitigate the Complexity Risk of you having to manage your own
memory, but in return perhaps give you fewer guarantees about the
\emph{performance} of your software. Again, there are times when you
can't accommodate this Operational Risk, but these are rare and usually
only affect a small portion of an entire software-system.

\hypertarget{protocols-and-types}{%
\subsection{Protocols And Types}\label{protocols-and-types}}

Whenever two components of a software system need to interact, they have
to establish a protocol for doing so. There are lots of different ways
this can work, but the simplest example I can think of is where some
component \textbf{a} calls some function \textbf{b}. e.g:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{function} \AttributeTok{b}\NormalTok{(a}\OperatorTok{,}\NormalTok{ b}\OperatorTok{,}\NormalTok{ c) }\OperatorTok{\{}
    \ControlFlowTok{return} \StringTok{"whatever"} \CommentTok{// do something here.}
\OperatorTok{\}}

\KeywordTok{function} \AttributeTok{a}\NormalTok{() }\OperatorTok{\{}
    \KeywordTok{var}\NormalTok{ bOut }\OperatorTok{=} \AttributeTok{b}\NormalTok{(}\StringTok{"one"}\OperatorTok{,} \StringTok{"two"}\OperatorTok{,} \StringTok{"three"}\NormalTok{)}\OperatorTok{;}
    \ControlFlowTok{return} \StringTok{"something "}\OperatorTok{+}\NormalTok{bOut}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

If component \textbf{b} then changes in some backwards-incompatible way,
say:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{function} \AttributeTok{b}\NormalTok{(a}\OperatorTok{,}\NormalTok{ b}\OperatorTok{,}\NormalTok{ c}\OperatorTok{,}\NormalTok{ d }\CommentTok{/* new parameter */}\NormalTok{) }\OperatorTok{\{}
    \ControlFlowTok{return} \StringTok{"whatever"} \CommentTok{// do something here.}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Then, we can say that the protocol has changed. This problem is so
common, so endemic to computing that we've had compilers that check
function arguments \href{https://en.wikipedia.org/wiki/Compiler}{since
the 1960's}. The point being is that it's totally possible for the
compiler to warn you about when a protocol within the program has
changed.

The same is basically true of
\href{https://en.wikipedia.org/wiki/Data_type}{Data Types}: whenever we
change the \textbf{Data Type}, we need to correct the usages of that
type. Note above, I've given the \texttt{javascript} example, but I'm
going to switch to \texttt{typescript} now:

\begin{verbatim}
interface BInput {
    a: string,
    b: string,
    c: string,
    d: string
}

function b(in: BInput): string {
    return "whatever" // do something here.
}
\end{verbatim}

Now, of course, there is a tradeoff: we \emph{mitigate} Complexity Risk,
because we define the protocols / types \emph{once only} in the program,
and ensure that usages all match the specification. But the tradeoff is
(as we can see in the \texttt{typescript} code) more
\emph{finger-typing}, which some people argue counts as Schedule Risk.

Nevertheless, compilers and type-checking are so prevalent in software
that clearly, you have to accept that in most cases, the trade-off has
been worth it: Even languages like \href{https://clojure.org}{Clojure}
have been retro-fitted with
\href{https://github.com/clojure/core.typed/wiki/User-Guide}{type
checkers}.

We're going to head into much more detail on this in the chapter on
Protocol Risk.

\hypertarget{space-and-time-complexity}{%
\subsection{Space and Time Complexity}\label{space-and-time-complexity}}

So far, we've looked at a couple of definitions of complexity in terms
of the codebase itself. However, in Computer Science there is a whole
branch of complexity theory devoted to how the software \emph{runs},
namely \href{https://en.wikipedia.org/wiki/Big_O_notation}{Big O
Complexity}.

Once running, an algorithm or data structure will consume space or
runtime dependent on it's characteristics. As with Garbage Collectors),
these characteristics can introduce Performance Risk which can easily
catch out the unwary. By and large, using off-the-shelf data structures
and algorithms helps, but you still need to know their performance
characteristics.

The \href{http://bigocheatsheet.com}{Big O Cheatsheet} is a wonderful
resource to investigate this further.

\hypertarget{concurrency-mutability}{%
\subsection{Concurrency / Mutability}\label{concurrency-mutability}}

Although modern languages include plenty of concurrency primitives,
(such as the
\href{https://docs.oracle.com/javase/9/docs/api/java/util/concurrent/package-summary.html}{java.util.concurrent}
libraries), concurrency is \emph{still} hard to get right.

\href{https://en.wikipedia.org/wiki/Race_condition}{Race conditions} and
\href{https://en.wikipedia.org/wiki/Deadlock}{Deadlocks} \emph{thrive}
in over-complicated concurrency designs: complexity issues are magnified
by concurrency concerns, and are also hard to test and debug.

Recently, languages such as Clojure have introduced
\href{https://en.wikipedia.org/wiki/Persistent_data_structure}{persistent
collections} to alleviate concurrency issues. The basic premise is that
any time you want to \emph{change} the contents of a collection, you get
given back a \emph{new collection}. So, any collection instance is
immutable once created. The tradeoff is again attendant Performance Risk
to mitigate Complexity Risk.

An important lesson here is that choice of language can reduce
complexity: and we'll come back to this in Software Dependency Risk.

\hypertarget{networking-security}{%
\subsection{Networking / Security}\label{networking-security}}

The last area I want to touch on here is networking. There are plenty of
Complexity Risk perils in \emph{anything} to do with networked code,
chief amongst them being error handling and (again) protocol evolution.

In the case of security considerations, exploits \emph{thrive} on the
complexity of your code, and the weaknesses that occur because of it. In
particular, Schneier's Law says, never implement your own crypto scheme:

\begin{quote}
``Anyone, from the most clueless amateur to the best cryptographer, can
create an algorithm that he himself can't break. It's not even hard.
What is hard is creating an algorithm that no one else can break, even
after years of analysis.'' -
\href{https://en.wikipedia.org/wiki/Bruce_Schneier\#Cryptography}{Bruce
Schneier, 1998}
\end{quote}

Luckily, most good languages include crypto libraries that you can
include to mitigate these Complexity Risks from your own code-base.

This is a strong argument for the use of libraries. But, when should you
use a library and when should you implement yourself? This is again
covered in the chapter on Software Dependency Risk.

tbd - next chapter.

costs associated with complexity risk

CHANGE is also more risky why?

\part{Preview}

book1/Part3.md practices/Estimates.md

\backmatter

\hypertarget{glossary}{%
\chapter{Glossary}\label{glossary}}

\hypertarget{abstraction-1}{%
\section{Abstraction}\label{abstraction-1}}

\hypertarget{feedback-loop}{%
\section{Feedback Loop}\label{feedback-loop}}

\hypertarget{goal-in-mind}{%
\section{Goal In Mind}\label{goal-in-mind}}

\hypertarget{internal-model}{%
\section{Internal Model}\label{internal-model}}

The most common use for Internal Model is to refer to the model of
reality that you or I carry around in our heads. You can regard the
concept of Internal Model as being what you \emph{know} and what you
\emph{think} about a certain situation.

Obviously, because we've all had different experiences, and our brains
are wired up differently, everyone will have a different Internal Model
of reality.

Alternatively, we can use the term Internal Model to consider other
viewpoints: - Within an organisation, we might consider the Internal
Model of a \emph{team of people} to be the shared knowledge, values and
working practices of that team. - Within a software system, we might
consider the Internal Model of a single processor, and what knowledge it
has of the world. - A codebase is a team's Internal Model written down
and encoded as software.

An internal model \emph{represents} reality: reality is made of atoms,
whereas the internal model is information.

\hypertarget{meet-reality}{%
\section{Meet Reality}\label{meet-reality}}

\hypertarget{risk}{%
\section{Risk}\label{risk}}

\hypertarget{attendant-risk}{%
\subsection{Attendant Risk}\label{attendant-risk}}

\hypertarget{hidden-risk}{%
\subsection{Hidden Risk}\label{hidden-risk}}

\hypertarget{mitigated-risk}{%
\subsection{Mitigated Risk}\label{mitigated-risk}}

\hypertarget{take-action}{%
\section{Take Action}\label{take-action}}

\end{document}  